{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# DenseNet and CSPNet\n",
    "\n",
    "**Author:** Varchita Lalwani<br>\n",
    "**Date created:** 27 June 2021<br>\n",
    "**Last modified:** 27 June 2021<br>\n",
    "**Description:** Training DenseNet, CSPNet and variants on Cifar10 Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!densenet_cspnet_variants.ipynb\n",
    "!\n",
    "!Automatically generated by Colaboratory.\n",
    "!\n",
    "!Original file is located at\n",
    "!    https://colab.research.google.com/drive/1ejnjgRzfo60FWXMp8_HyzQJ_5kBLIdwd\n",
    "!\n",
    "!##DenseNet\n",
    "!\n",
    "!Densely Connected Convolutional Networks: - With dense connection, fewer parameters and high accuracy are achieved compared with ResNet and Pre-Activation ResNet. In Standard ConvNet, input image goes through multiple convolution and obtain high-level features. In ResNet, identity mapping is proposed to promote the gradient propagation. Element-wise addition is used. An advantage of ResNets is that the gradient can flow directly through the identity function from later layers to the earlier layers. However, the identity function and the output of layers are combined by summation, which may impede the information flow in the network. To further improve the information\n",
    "!flow between layers author of paper propose a different connectivity\n",
    "!pattern: They introduce direct connections from any layer to all subsequent layers. Each layer has access to all the preceding feature-maps in its block and, therefore, to the network's \"collective knowledge\". Since each layer receives feature maps from all preceding layers, network can be thinner and compact, i.e. number of channels can be fewer. The growth rate k is the additional number of channels for each layer. So, it have higher computational efficiency and memory efficiency.\n",
    "!\n",
    "!####Bottleneck Layers\n",
    "!A 1\u00c3\u20141 convolution can be introduced as bottleneck layer before each 3\u00c3\u20143 convolution to reduce the number of input feature-maps, and thus to improve computational efficiency.\n",
    "!\n",
    "!So, dense block will have convolution with kernel size 1 followed by convolution with kernel size 3.\n",
    "!\n",
    "!DenseNet-B will have BN-ReLU-Conv(1\u00c3\u20141) BN-ReLU-Conv(3\u00c3\u20143).\n",
    "!\n",
    "!####Composite function\n",
    "!Three consecutive operations: batch normalization (BN), followed by a rectified linear unit (ReLU) and a 3 \u00c3\u2014 3 convolution (Conv).\n",
    "!\n",
    "!####Pooling Layers\n",
    "!The concatenation operation used is not viable when the size of feature-maps changes. However, an essential part of convolutional networks is down-sampling layers that change the size of feature-maps. To facilitate down-sampling authors divide the network into multiple densely connected dense blocks; refered to layers between blocks as transition layers, which do convolution and pooling. The transition layers used in experiments consist of a batch normalization layer and an 1\u00c3\u20141 convolutional layer followed by a 2\u00c3\u20142 average pooling layer\n",
    "!\n",
    "!####Compression\n",
    "!To further improve model compactness, the number of feature-maps at transition\n",
    "!layers can be reduced. If a dense block contains m feature-maps, then let\n",
    "!the following transition layer generate floor_function(theta) output featuremaps, where theta can go maximum to 1. theta is referred to as the compression factor. When theta = 1, the number of feature-maps across transition layers remains unchanged.\n",
    "!\n",
    "!When both the bottleneck and transition layers with theta < 1\n",
    "!are used, model is reffered as DenseNet-BC.\n",
    "!\n",
    "!DenseNets utilize parameters more efficiently than alternative architectures (in particular, ResNets). The DenseNetBC with bottleneck structure and dimension reduction at transition layers is particularly parameter-efficient\n",
    "!\n",
    "!##CSPNet\n",
    "!Cross Stage Partial DenseNet\n",
    "!A stage of CSPDenseNet is composed of a partial dense block and a partial transition layer.  In a partial dense block, the feature maps of the base layer in a stage are split into two parts through channel. the former is directly linked to the end of the stage, and the latter will go through a dense block. All steps involved in a partial transition layer are as follows:-  First, the output of dense layers, will undergo a transition layer. Second, the output of this transition layer,  will be concatenated with first part and undergo another transition layer, and then generate output final output.\n",
    "!\n",
    "!The proposed CSPDenseNet preserves the advantages of DenseNet\u00e2\u20ac\u2122s feature reuse characteristics, but at the same time prevents an excessively amount of duplicate gradient information by truncating the gradient flow. This idea is realized by designing a hierarchical feature fusion strategy and used in a partial transition layer.\n",
    "!\n",
    "!####Partial Dense Block\n",
    "!The purpose of designing partial dense blocks is to\n",
    "!1.) increase gradient path: Through the split and merge strategy, the number of gradient paths can be doubled. Because of the cross-stage strategy, one can alleviate the disadvantages caused by using explicit feature map copy for concatenation.\n",
    "!2.) balance computation of each layer: usually, the channel number in the base layer of a DenseNet is much larger than the growth rate. Since the base layer channels involved in the dense layer operation in a partial dense block account for only half of the original number, it can effectively solve nearly half of the computational bottleneck\n",
    "!3.) reduce memory traffic:\n",
    "!\n",
    "!####Partial Transition Layer.\n",
    "!The purpose of designing partial transition layers is to maximize the difference of gradient combination. The partial transition layer is a hierarchical feature fusion mechanism, which uses the strategy of truncating the gradient flow to prevent distinct layers from learning duplicate gradient information.\n",
    "!\n",
    "!##To summarize: -\n",
    "!1) DenseNet: - Base Layer -> Denseblock -> Transition Layer -> concat base layer with output of transition layer\n",
    "!2) CSPDenseNet: - Divide base layer to part1 and part2 via channenls\n",
    "!Part2 -> Denseblock -> Transition Layer -> concate part1, part2 ->   Transition Layer\n",
    "!3) Variant of CSPNet (Fusion First): - Divide base layer to part1 and part2 via channenls\n",
    "!Part2 -> Denseblock -> concat part1 with output of denseblock -> Transition layer.\n",
    "!4) Variant of CSPNet (Fusion Last): - Divide base layer to part1 and part2 via channenls\n",
    "!Part2 -> Denseblock -> Transition Layer -> concate part1, part2\n",
    "!\n",
    "!CSP (fusion first) means to concatenate the feature maps generated by two parts, and then do transition operation. If this strategy is adopted, a large amount of gradient information will be reused. As to the CSP (fusion last) strategy, the output from the dense block will go through the transition layer and then do concatenation with the feature map coming from part 1. If one goes with the CSP (fusion last) strategy, the gradient information will not be reused since the gradient flow is truncated.\n",
    "!\n",
    "!DenseNet: - https://arxiv.org/pdf/1608.06993.pdf\n",
    "!CSPNet: - https://arxiv.org/pdf/1911.11929.pdf\n",
    "!\n",
    "!Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "import tensorflow.keras.activations as act\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D,\n",
    "    Input,\n",
    "    MaxPool2D,\n",
    "    Concatenate,\n",
    "    AveragePooling2D,\n",
    "    GlobalAveragePooling2D,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    "    Reshape,\n",
    "    Flatten,\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Get the data (cifar10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "X_train, x_val, Y_train, y_val = split(x_train, y_train, test_size=0.1)\n",
    "print(\n",
    "    \"X_train.shape, Y_train.shape, x_test.shape, y_test.shape, x_val.shape, Y_train.shape, y_val.shape\"\n",
    ")\n",
    "print(\n",
    "    X_train.shape,\n",
    "    Y_train.shape,\n",
    "    x_test.shape,\n",
    "    y_test.shape,\n",
    "    x_val.shape,\n",
    "    Y_train.shape,\n",
    "    y_val.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "train_norm = X_train.astype(\"float32\")\n",
    "test_norm = x_test.astype(\"float32\")\n",
    "val_norm = x_val.astype(\"float32\")\n",
    "train_norm /= 255.0\n",
    "test_norm /= 255.0\n",
    "val_norm /= 255.0\n",
    "del X_train, x_test, x_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Prepare the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "train_label = to_categorical(Y_train)\n",
    "test_label = to_categorical(y_test)\n",
    "val_label = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Activation function: - Mish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def mish(x):\n",
    "    x = x * (act.tanh(act.softplus(x)))\n",
    "    return x\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Define Convolution block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def conv_block(inps, convs):\n",
    "    x = inps\n",
    "    for conv in convs:\n",
    "        x = Conv2D(\n",
    "            conv[\"filter\"],\n",
    "            conv[\"kernel\"],\n",
    "            conv[\"strides\"],\n",
    "            conv[\"padding\"],\n",
    "            name=\"conv_\" + str(conv[\"layer_ids\"]),\n",
    "        )(x)\n",
    "        x = mish(x)\n",
    "    return x\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "In place of softmax, I have used Network in Network block, which helps in reducing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def nin_block(inps, filter, ker):\n",
    "    x = inps\n",
    "    x = Conv2D(filter, ker, padding=\"same\", name=\"nin_\" + str(0))(x)\n",
    "    x = mish(x)\n",
    "    x = Conv2D(filter, 1, padding=\"same\", name=\"nin_\" + str(1))(x)\n",
    "    x = mish(x)\n",
    "    x = Conv2D(filter, 1, padding=\"same\", name=\"nin_\" + str(2))(x)\n",
    "    x = mish(x)\n",
    "    return x\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Define Dense Block (1x1 followed by 3x3 convolutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# for densenet-bc uncomment the lines of this block\n",
    "def dense_block(inps, filter, times, id):\n",
    "    for time in range(0, times):\n",
    "        # inps = BatchNormalization()(inps)\n",
    "        shape = inps.shape\n",
    "        part1 = inps\n",
    "        part2 = part1\n",
    "        part2 = Conv2D(filter[0], 1, padding=\"same\")(part2)\n",
    "        part2 = mish(part2)\n",
    "        # part2 = BatchNormalization()(part2)\n",
    "        part2 = Conv2D(filter[1], 3, padding=\"same\")(part2)\n",
    "        part2 = mish(part2)\n",
    "        inps = Concatenate()([part1, part2])\n",
    "    return inps\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Define Dense Block CSPNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# for cspnet and fusion last the block remains same\n",
    "# for fusion first comment part2 = Conv2D(filter[2], 1, padding='same')(part2) and part2 = mish(part2)\n",
    "def dense_block_cspnet(inps, partition, filter, times, id):\n",
    "    shape = inps.shape\n",
    "    features = shape[3] - partition\n",
    "    part1 = inps\n",
    "    part1 = inps[:, :, :, 0:features]\n",
    "    part2 = inps[:, :, :, features:]\n",
    "    for time in range(0, times):\n",
    "        part2 = Conv2D(filter[0], 1, padding=\"same\")(part2)\n",
    "        part2 = mish(part2)\n",
    "        part2 = Conv2D(filter[1], 3, padding=\"same\")(part2)\n",
    "        part2 = mish(part2)\n",
    "\n",
    "    part2 = Conv2D(filter[2], 1, padding=\"same\")(part2)\n",
    "    part2 = mish(part2)\n",
    "    inps = Concatenate()([part1, part2])\n",
    "    return inps\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Define DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# for densenet block remains same\n",
    "# for densenet-b also the block remains same\n",
    "# for densenet-bc, while calling all conv_block,\n",
    "# change filters to half of used in dense_block\n",
    "# In this case change 48/2 i.e 24\n",
    "# np chnage to conv_block with layer_id = 0\n",
    "def densenet():\n",
    "    shp = train_norm.shape\n",
    "    input_image = Input(shape=(shp[1], shp[2], shp[3]))\n",
    "    # Layer 0 => 1\n",
    "    x = conv_block(\n",
    "        input_image,\n",
    "        [{\"filter\": 32, \"kernel\": 7, \"padding\": \"same\", \"strides\": 2, \"layer_ids\": 0}],\n",
    "    )\n",
    "    x = MaxPool2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # Dense block 1\n",
    "    x = dense_block(x, [32, 32], 10, \"dense_block1_\")\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        [{\"filter\": 32, \"kernel\": 1, \"strides\": 1, \"padding\": \"same\", \"layer_ids\": 2}],\n",
    "    )\n",
    "    x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # dense block 2\n",
    "    x = dense_block(x, [32, 32], 10, \"dense_block2_\")\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        [{\"filter\": 32, \"kernel\": 1, \"strides\": 1, \"padding\": \"same\", \"layer_ids\": 3}],\n",
    "    )\n",
    "    x = AveragePooling2D(pool_size=(2, 2), strides=2)(x)\n",
    "\n",
    "    # dense block 3\n",
    "    x = dense_block(x, [32, 32], 10, \"dense_block3_\")\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        [{\"filter\": 32, \"kernel\": 1, \"strides\": 1, \"padding\": \"same\", \"layer_ids\": 4}],\n",
    "    )\n",
    "    x = AveragePooling2D(pool_size=(2, 2), strides=2)(x)\n",
    "\n",
    "    # nin block\n",
    "    x = nin_block(x, num_clas, 3)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Reshape((1, 1, num_clas))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Model(inputs=input_image, outputs=x)\n",
    "    return x\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Define CSPDenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# for cspnet and fusion first the block remains same\n",
    "# for fusion last comment the lines calling conv_block\n",
    "# so here 3 lines calling conv_block will be commented for fusion last\n",
    "def cspnet():\n",
    "    shp = train_norm.shape\n",
    "    input_image = Input(shape=(shp[1], shp[2], shp[3]))\n",
    "    # Layer 0 => 1\n",
    "    x = conv_block(\n",
    "        input_image,\n",
    "        [{\"filter\": 32, \"kernel\": 7, \"padding\": \"same\", \"strides\": 2, \"layer_ids\": 0}],\n",
    "    )\n",
    "    x = MaxPool2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # Dense block 1\n",
    "    x = dense_block_cspnet(x, 24, [64, 64, 64], 20, \"dense_block1_\")\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        [{\"filter\": 64, \"kernel\": 1, \"strides\": 1, \"padding\": \"same\", \"layer_ids\": 2}],\n",
    "    )\n",
    "    x = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # dense block 2\n",
    "    x = dense_block_cspnet(x, 24, [64, 64, 64], 20, \"dense_block2_\")\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        [{\"filter\": 64, \"kernel\": 1, \"strides\": 1, \"padding\": \"same\", \"layer_ids\": 3}],\n",
    "    )\n",
    "    x = AveragePooling2D(pool_size=(2, 2), strides=2)(x)\n",
    "\n",
    "    # dense block 3\n",
    "    x = dense_block_cspnet(x, 24, [64, 64, 64], 20, \"dense_block3_\")\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        [{\"filter\": 64, \"kernel\": 1, \"strides\": 1, \"padding\": \"same\", \"layer_ids\": 4}],\n",
    "    )\n",
    "    x = AveragePooling2D(pool_size=(2, 2), strides=2)(x)\n",
    "\n",
    "    # nin block\n",
    "    x = nin_block(x, num_clas, 3)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Reshape((1, 1, num_clas))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Model(inputs=input_image, outputs=x)\n",
    "    return x\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(\n",
    "    func, train_norm, train_label, num_epochs, lr, val_norm, val_label, batch_size\n",
    "):\n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "    loss = CategoricalCrossentropy(from_logits=True)\n",
    "    model = func()\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "    model.fit(\n",
    "        train_norm,\n",
    "        train_label,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=(val_norm, val_label),\n",
    "        callbacks=callbacks,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    return model\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Train the model\n",
    "Change the name of the function while calling train:-\n",
    "densenet, cspnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "lr = 0.01\n",
    "num_epochs = 1\n",
    "num_clas = 10\n",
    "\n",
    "\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch == 11:\n",
    "        lr = 0.001\n",
    "    return lr\n",
    "\n",
    "\n",
    "callbacks = [LearningRateScheduler(lr_scheduler)]\n",
    "model = train(\n",
    "    densenet, train_norm, train_label, num_epochs, lr, val_norm, val_label, batch_size\n",
    ")\n",
    "model.evaluate(test_norm, test_label)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "..\\examples\\vision\\densenet_cspnet_variants",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}